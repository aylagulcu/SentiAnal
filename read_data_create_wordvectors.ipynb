{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus reader:\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import os\n",
    "root = './Confs_newline/Conf1/'\n",
    "from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n",
    "reader = CategorizedPlaintextCorpusReader(root, r'.*\\.txt', cat_pattern=r'(\\w+)/*', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kiz', 'kork', 'mutlu']\n",
      "['kiz.txt', 'kork.txt', 'mutlu.txt']\n"
     ]
    }
   ],
   "source": [
    "print(reader.categories())\n",
    "print(reader.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First, tokenize Punctuation: \n",
    "# create a token dictionary:\n",
    "punc_dict= {'.':'||PERIOD||', ',': '||COMMA||', '\"': '||QUOTATION_MARK||', ';': '||SEMICOLON||',\n",
    "                '!': '||EXCLAMATION_MARK||', '?': '||QUESTION_MARK||', '(': '||LEFT_PAREN||',\n",
    "                ')': '||RIGHT_PAREN||', '?': '||QUESTION_MARK||', \n",
    "                '\\n': '||NEW_LINE||', '-': '||DASH||'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenize_whole_tweets(text): # raw text --> whole tweets file content\n",
    "    for key, token in punc_dict.items():\n",
    "        text = text.replace(key, ' {} '.format(token))\n",
    "\n",
    "    sentences= []\n",
    "    for line in text.split('||NEW_LINE||'):\n",
    "        line= line.strip()\n",
    "        sentences.append(line)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text=[]\n",
    "labels= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label,file_name in zip(reader.categories(), reader.fileids()):\n",
    "    sentences= sent_tokenize_whole_tweets(reader.raw(file_name)) # --> this should return a list of contents\n",
    "    labels.extend([label for i in sentences])\n",
    "    all_text.extend([i.lower() for i in sentences])\n",
    "\n",
    "# Now, we have all tweets in all_text list!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Transforming Text into Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 7426 \n",
      "Number of tweets: 2170\n"
     ]
    }
   ],
   "source": [
    "word_counts={}\n",
    "for i in range(len(all_text)):\n",
    "    for word in all_text[i].split(\" \"):\n",
    "        word_counts[word] = word_counts.get(word,0) +1\n",
    "\n",
    "vocab = set(word_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Number of unique words: {} \".format(vocab_size))\n",
    "print(\"Number of tweets: {}\".format(len(all_text)))\n",
    "assert len(all_text)== len(labels), \"Each tweet should have a label.\"\n",
    "\n",
    "sorted_word_counts= sorted(word_counts, key= word_counts.get, reverse= True)\n",
    "\n",
    "int_to_vocab= {ii: word for ii,word in enumerate(sorted_word_counts)}\n",
    "vocab_to_int= {word: ii for ii, word in int_to_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hayır çok severim diziyi yani aşırı kızdım'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_vocab[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing Noise in the Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling\n",
    "\n",
    "Words that show up often such as \"the\", \"of\", and \"for\" don't provide much context to the nearby words. If we discard some of them, we can remove some of the noise from our data and in return get faster training and better representations. This process is called subsampling by Mikolov. For each word $w_i$ in the training set, we'll discard it with probability given by \n",
    "\n",
    "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} $$\n",
    "\n",
    "where $t$ is a threshold parameter and $f(w_i)$ is the frequency of word $w_i$ in the total dataset.\n",
    "\n",
    "$$ P(0) = 1 - \\sqrt{\\frac{1*10^{-5}}{1*10^6/16*10^6}} = 0.98735 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4175, 2857, 2991, 4747]\n"
     ]
    }
   ],
   "source": [
    "int_words = [vocab_to_int[word] for word in vocab]\n",
    "print(int_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2139, 2140, 142, 2143, 97, 2144, 1225, 2145, 2146, 842, 632, 1226, 172, 2148, 18, 2149, 406, 2150, 844, 1229, 2152, 2153, 636, 2154, 2155, 1231, 2156, 2157, 63, 2159]\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-4\n",
    "\n",
    "word_counts_intwords = {vocab_to_int[word]:count for word,count in word_counts.items()}\n",
    "total_count = vocab_size\n",
    "freqs = {word: count/total_count for word, count in word_counts_intwords.items()}\n",
    "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts_intwords}\n",
    "# discard some frequent words, according to the subsampling equation\n",
    "# create a new list of words for training\n",
    "\n",
    "selected_words = [word for word in word_counts_intwords if random.random() < (1 - p_drop[word])]\n",
    "print(selected_words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5552"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'çok kızdım ne kadar ayıp şey'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_text)):\n",
    "    line_split= all_text[i].split(\" \")\n",
    "    line_split_reduced= [word for word in line_split if vocab_to_int[word] in selected_words]\n",
    "    all_text[i]= ' '.join(line_split_reduced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete empty tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2170"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets after reducing some noise: 2059\n"
     ]
    }
   ],
   "source": [
    "all_text= [line for line in all_text if len(line)>0]\n",
    "labels= [labels[i] for i in range(len(all_text)) if len(all_text[i])>0]\n",
    "\n",
    "print(\"Number of tweets after reducing some noise: {}\".format(len(all_text)))\n",
    "assert len(all_text)== len(labels), \"Each tweet should have a label.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the words\n",
    "The embedding lookup requires that we pass in integers to our network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Then we can convert each of our reviews into integers so they can be passed into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 5552 \n"
     ]
    }
   ],
   "source": [
    "word_counts={}\n",
    "for i in range(len(all_text)):\n",
    "    for word in all_text[i].split(\" \"):\n",
    "        word_counts[word] = word_counts.get(word,0) +1\n",
    "\n",
    "vocab = set(word_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "print(\"Number of unique words: {} \".format(vocab_size))\n",
    "\n",
    "sorted_word_counts= sorted(word_counts, key= word_counts.get, reverse= True)\n",
    "\n",
    "int_to_vocab= {ii: word for ii,word in enumerate(sorted_word_counts, 1)} #start from 1.\n",
    "vocab_to_int= {word: ii for ii, word in int_to_vocab.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use the dict to tokenize each review in reviews_split\n",
    "## store the tokenized reviews in reviews_ints\n",
    "encoded_tweets = []\n",
    "for tweet in all_text:\n",
    "    encoded_tweets.append([vocab_to_int[word] for word in tweet.split(\" \")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_label(l):\n",
    "    if l== 'kiz':\n",
    "        return 0\n",
    "    elif l== 'kork':\n",
    "        return 1\n",
    "    return 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = np.array([map_label(label) for label in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding sequences\n",
    "\n",
    "To deal with both short and very long reviews, we'll pad or truncate all our reviews to a specific length. For reviews shorter than some seq_length, we'll pad with 0s. For reviews longer than seq_length, we can truncate them to the first seq_length words. A good seq_length, in this case, is 200.\n",
    "\n",
    "    Exercise: Define a function that returns an array features that contains the padded data, of a standard size, that we'll pass to the network.\n",
    "\n",
    "        The data should come from review_ints, since we want to feed integers to the network.\n",
    "        Each row should be seq_length elements long.\n",
    "        For reviews shorter than seq_length words, left pad with 0s. That is, if the review is ['best', 'movie', 'ever'], [117, 18, 128] as integers, the row will look like [0, 0, 0, ..., 0, 117, 18, 128].\n",
    "        For reviews longer than seq_length, use only the first seq_length words as the feature vector.\n",
    "\n",
    "As a small example, if the seq_length=10 and an input review is:\n",
    "\n",
    "[117, 18, 128]\n",
    "\n",
    "The resultant, padded sequence should be:\n",
    "\n",
    "[0, 0, 0, 0, 0, 0, 0, 117, 18, 128]\n",
    "\n",
    "Your final features array should be a 2D array, with as many rows as there are reviews, and as many columns as the specified seq_length.\n",
    "\n",
    "This isn't trivial and there are a bunch of ways to do this. But, if you're going to be building your own deep learning networks, you're going to have to get used to preparing your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_lenght= max([len(i) for i in encoded_tweets])\n",
    "max_lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_lenght= min([len(i) for i in encoded_tweets])\n",
    "min_lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation!\n",
    "\n",
    "seq_length = 10\n",
    "\n",
    "features = pad_features(encoded_tweets, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(features)==len(encoded_tweets), \"Your features should have as many rows as reviews.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2059"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
