{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process all the data\n",
    "\n",
    "Running the code cell below will pre-process all the data and save it to file. You're encouraged to lok at the code for `preprocess_and_save_data` in the `helpers.py` file to see what it's doing in detail, but you do not need to change this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus reader:\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import os\n",
    "root = './Confs_newline/Conf2/'\n",
    "from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n",
    "reader = CategorizedPlaintextCorpusReader(root, r'.*\\.txt', cat_pattern=r'(\\w+)/*', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kiz', 'kork', 'mutlu', 'notr', 'uzul']\n",
      "['kiz.txt', 'kork.txt', 'mutlu.txt', 'notr.txt', 'uzul.txt']\n"
     ]
    }
   ],
   "source": [
    "print(reader.categories())\n",
    "print(reader.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First, tokenize Punctuation: \n",
    "# create a token dictionary:\n",
    "punc_dict= {'.':'||PERIOD||', ',': '||COMMA||', '\"': '||QUOTATION_MARK||', ';': '||SEMICOLON||',\n",
    "                '!': '||EXCLAMATION_MARK||', '?': '||QUESTION_MARK||', '(': '||LEFT_PAREN||',\n",
    "                ')': '||RIGHT_PAREN||', '?': '||QUESTION_MARK||', \n",
    "                '\\n': '||NEW_LINE||', '-': '||DASH||'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenize_whole_tweets(text): # raw text --> whole tweets file content\n",
    "    for key, token in punc_dict.items():\n",
    "        text = text.replace(key, ' {} '.format(token))\n",
    "\n",
    "    sentences= []\n",
    "    for line in text.split('||NEW_LINE||'):\n",
    "        line= line.strip()\n",
    "        sentences.append(line)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text=[]\n",
    "labels= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3317\n",
      "3317\n"
     ]
    }
   ],
   "source": [
    "for label,file_name in zip(reader.categories(), reader.fileids()):\n",
    "    sentences= sent_tokenize_whole_tweets(reader.raw(file_name)) # --> this should return a list of contents\n",
    "    labels.extend([label for i in sentences])\n",
    "    all_text.extend([i.lower() for i in sentences])\n",
    "print(len(labels))\n",
    "print(len(all_text))\n",
    "# Now, we have all tweets in all_text list!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Transforming Text into Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 3704 \n"
     ]
    }
   ],
   "source": [
    "word_counts={}\n",
    "for i in range(len(all_text)):\n",
    "    for word in all_text[i].split(\" \"):\n",
    "        word_counts[word] = word_counts.get(word,0) +1\n",
    "\n",
    "vocab = set(word_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "print(\"Number of unique words: {} \".format(vocab_size))\n",
    "\n",
    "sorted_word_counts= sorted(word_counts, key= word_counts.get, reverse= True)\n",
    "\n",
    "int_to_vocab= {ii: word for ii,word in enumerate(sorted_word_counts)}\n",
    "vocab_to_int= {word: ii for ii, word in int_to_vocab.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hayır çok sev dizi yani aşırı kız'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'çok'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_vocab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "696"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts['çok']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing Noise in the Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling\n",
    "\n",
    "Words that show up often such as \"the\", \"of\", and \"for\" don't provide much context to the nearby words. If we discard some of them, we can remove some of the noise from our data and in return get faster training and better representations. This process is called subsampling by Mikolov. For each word $w_i$ in the training set, we'll discard it with probability given by \n",
    "\n",
    "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} $$\n",
    "\n",
    "where $t$ is a threshold parameter and $f(w_i)$ is the frequency of word $w_i$ in the total dataset.\n",
    "\n",
    "$$ P(0) = 1 - \\sqrt{\\frac{1*10^{-5}}{1*10^6/16*10^6}} = 0.98735 $$\n",
    "\n",
    "I'm going to leave this up to you as an exercise. Check out my solution to see how I did it.\n",
    "\n",
    "> **Exercise:** Implement subsampling for the words in `int_words`. That is, go through `int_words` and discard each word given the probablility $P(w_i)$ shown above. Note that $P(w_i)$ is the probability that a word is discarded. Assign the subsampled data to `train_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_int['her']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 1297, 1931, 1407, 2910]\n"
     ]
    }
   ],
   "source": [
    "int_words = [vocab_to_int[word] for word in vocab]\n",
    "print(int_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[263, 1457, 2022, 360, 2023, 496, 1007, 2028, 443, 322, 2029, 1008, 537, 538, 1462, 539, 2037, 2039, 310, 403, 774, 344, 1477, 1479, 2061, 506, 2068, 778, 192, 2072]\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-5\n",
    "\n",
    "word_counts_intwords = {vocab_to_int[word]:count for word,count in word_counts.items()}\n",
    "total_count = vocab_size\n",
    "freqs = {word: count/total_count for word, count in word_counts_intwords.items()}\n",
    "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts_intwords}\n",
    "# discard some frequent words, according to the subsampling equation\n",
    "# create a new list of words for training\n",
    "\n",
    "\n",
    "train_words = [word for word in word_counts_intwords if random.random() < (1 - p_drop[word])]\n",
    "print(train_words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while training the network, do not feed the words not in this list!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
