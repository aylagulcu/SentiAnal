{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process all the data\n",
    "\n",
    "Running the code cell below will pre-process all the data and save it to file. You're encouraged to lok at the code for `preprocess_and_save_data` in the `helpers.py` file to see what it's doing in detail, but you do not need to change this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus reader:\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import os\n",
    "root = './Confs_newline/Conf2/'\n",
    "from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n",
    "reader = CategorizedPlaintextCorpusReader(root, r'.*\\.txt', cat_pattern=r'(\\w+)/*', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kiz', 'kork', 'mutlu', 'notr', 'uzul']\n",
      "['kiz.txt', 'kork.txt', 'mutlu.txt', 'notr.txt', 'uzul.txt']\n"
     ]
    }
   ],
   "source": [
    "print(reader.categories())\n",
    "print(reader.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First, tokenize Punctuation: \n",
    "# create a token dictionary:\n",
    "punc_dict= {'.':'||PERIOD||', ',': '||COMMA||', '\"': '||QUOTATION_MARK||', ';': '||SEMICOLON||',\n",
    "                '!': '||EXCLAMATION_MARK||', '?': '||QUESTION_MARK||', '(': '||LEFT_PAREN||',\n",
    "                ')': '||RIGHT_PAREN||', '?': '||QUESTION_MARK||', \n",
    "                '\\n': '||NEW_LINE||', '-': '||DASH||'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenize_whole_tweets(text): # raw text --> whole tweets file content\n",
    "    for key, token in punc_dict.items():\n",
    "        text = text.replace(key, ' {} '.format(token))\n",
    "\n",
    "    sentences= []\n",
    "    for line in text.split('||NEW_LINE||'):\n",
    "        line= line.strip()\n",
    "        sentences.append(line)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text=[]\n",
    "labels= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3317\n",
      "3317\n"
     ]
    }
   ],
   "source": [
    "for label,file_name in zip(reader.categories(), reader.fileids()):\n",
    "    sentences= sent_tokenize_whole_tweets(reader.raw(file_name)) # --> this should return a list of contents\n",
    "    labels.extend([label for i in sentences])\n",
    "    all_text.extend([i.lower() for i in sentences])\n",
    "print(len(labels))\n",
    "print(len(all_text))\n",
    "# Now, we have all tweets in all_text list!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Transforming Text into Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 3704 \n"
     ]
    }
   ],
   "source": [
    "word_counts={}\n",
    "for i in range(len(all_text)):\n",
    "    for word in all_text[i].split(\" \"):\n",
    "        word_counts[word] = word_counts.get(word,0) +1\n",
    "\n",
    "vocab = set(word_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "print(\"Number of unique words: {} \".format(vocab_size))\n",
    "\n",
    "sorted_word_counts= sorted(word_counts, key= word_counts.get, reverse= True)\n",
    "\n",
    "int_to_vocab= {ii: word for ii,word in enumerate(sorted_word_counts)}\n",
    "vocab_to_int= {word: ii for ii, word in int_to_vocab.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hayır çok sev dizi yani aşırı kız'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'çok'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_vocab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "696"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts['çok']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing Noise in the Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling\n",
    "\n",
    "Words that show up often such as \"the\", \"of\", and \"for\" don't provide much context to the nearby words. If we discard some of them, we can remove some of the noise from our data and in return get faster training and better representations. This process is called subsampling by Mikolov. For each word $w_i$ in the training set, we'll discard it with probability given by \n",
    "\n",
    "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} $$\n",
    "\n",
    "where $t$ is a threshold parameter and $f(w_i)$ is the frequency of word $w_i$ in the total dataset.\n",
    "\n",
    "$$ P(0) = 1 - \\sqrt{\\frac{1*10^{-5}}{1*10^6/16*10^6}} = 0.98735 $$\n",
    "\n",
    "I'm going to leave this up to you as an exercise. Check out my solution to see how I did it.\n",
    "\n",
    "> **Exercise:** Implement subsampling for the words in `int_words`. That is, go through `int_words` and discard each word given the probablility $P(w_i)$ shown above. Note that $P(w_i)$ is the probability that a word is discarded. Assign the subsampled data to `train_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_int['her']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 1444, 2840, 2892, 2969]\n"
     ]
    }
   ],
   "source": [
    "int_words = [vocab_to_int[word] for word in vocab]\n",
    "print(int_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020, 688, 27, 1178, 1456, 768, 263, 398, 291, 1179, 689, 418, 2021, 381, 1180, 1457, 636, 1181, 54, 188, 2022, 72, 578, 360, 2023, 2024, 1006, 3, 1458, 535]\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-3\n",
    "\n",
    "word_counts_intwords = {vocab_to_int[word]:count for word,count in word_counts.items()}\n",
    "total_count = vocab_size\n",
    "freqs = {word: count/total_count for word, count in word_counts_intwords.items()}\n",
    "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts_intwords}\n",
    "# discard some frequent words, according to the subsampling equation\n",
    "# create a new list of words for training\n",
    "\n",
    "train_words = [word for word in word_counts_intwords if random.random() < (1 - p_drop[word])]\n",
    "print(train_words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3272"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3704"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while training the network, do not feed the words not in this list!\n",
    "# clean the tweets from those words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'çok kız ||exclamation_mark||  ne kadar ayıp şey'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_text)):\n",
    "    for word in all_text[i].split(\" \"):\n",
    "        word_counts[word] = word_counts.get(word,0) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "çok kız ||exclamation_mark||  ne kadar ayıp şey\n",
      "['çok', 'kız', '||exclamation_mark||', '', 'ne', 'kadar', 'ayıp', 'şey']\n",
      "['||exclamation_mark||', 'ayıp', 'şey']\n",
      "||exclamation_mark|| ayıp şey\n",
      "***\n",
      "koku çok kız çünkü ayva ben tek arkadaş  insan var kendi git gibi tek arkadaş götür yan\n",
      "['koku', 'çok', 'kız', 'çünkü', 'ayva', 'ben', 'tek', 'arkadaş', '', 'insan', 'var', 'kendi', 'git', 'gibi', 'tek', 'arkadaş', 'götür', 'yan']\n",
      "['koku', 'ayva', 'götür']\n",
      "koku ayva götür\n",
      "***\n",
      "hayır çok sev dizi yani aşırı kız\n",
      "['hayır', 'çok', 'sev', 'dizi', 'yani', 'aşırı', 'kız']\n",
      "['hayır']\n",
      "hayır\n",
      "***\n",
      "buna kız de evet ona kız ney kız kızneg sen mantık göre belirleneg kusur bakneg\n",
      "['buna', 'kız', 'de', 'evet', 'ona', 'kız', 'ney', 'kız', 'kızneg', 'sen', 'mantık', 'göre', 'belirleneg', 'kusur', 'bakneg']\n",
      "['buna', 'ona', 'kızneg', 'mantık', 'göre', 'belirleneg', 'bakneg']\n",
      "buna ona kızneg mantık göre belirleneg bakneg\n",
      "***\n",
      "kız ve çok üzül\n",
      "['kız', 've', 'çok', 'üzül']\n",
      "[]\n",
      "\n",
      "***\n",
      "kız bu geç ama aynı zaman kır bu nasıl geç\n",
      "['kız', 'bu', 'geç', 'ama', 'aynı', 'zaman', 'kır', 'bu', 'nasıl', 'geç']\n",
      "[]\n",
      "\n",
      "***\n",
      "of ben yet çok kız karakter  ne güncelneg yap herkes vere zor ne adam\n",
      "['of', 'ben', 'yet', 'çok', 'kız', 'karakter', '', 'ne', 'güncelneg', 'yap', 'herkes', 'vere', 'zor', 'ne', 'adam']\n",
      "['yet', 'güncelneg', 'vere']\n",
      "yet güncelneg vere\n",
      "***\n",
      "bak kendi çok kız şu sen muhatap al için oku için bekle\n",
      "['bak', 'kendi', 'çok', 'kız', 'şu', 'sen', 'muhatap', 'al', 'için', 'oku', 'için', 'bekle']\n",
      "['muhatap', 'al', 'bekle']\n",
      "muhatap al bekle\n",
      "***\n",
      "sonra durul küs kız biz bu kadar gönül bu takım sar o herkes met et futbol defans ton\n",
      "['sonra', 'durul', 'küs', 'kız', 'biz', 'bu', 'kadar', 'gönül', 'bu', 'takım', 'sar', 'o', 'herkes', 'met', 'et', 'futbol', 'defans', 'ton']\n",
      "['durul', 'biz', 'gönül', 'sar', 'met', 'defans', 'ton']\n",
      "durul biz gönül sar met defans ton\n",
      "***\n",
      "evet çok sev hem de gül bir durum görneg sine çok kız\n",
      "['evet', 'çok', 'sev', 'hem', 'de', 'gül', 'bir', 'durum', 'görneg', 'sine', 'çok', 'kız']\n",
      "['bir', 'sine']\n",
      "bir sine\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "for i in range(10): #len(all_text)):\n",
    "    print(all_text[i])\n",
    "    line_split= all_text[i].split(\" \")\n",
    "    print(line_split)\n",
    "    line_split_reduced= [word for word in line_split if vocab_to_int[word] in train_words]\n",
    "    print(line_split_reduced)\n",
    "    all_text[i]= ' '.join(line_split_reduced)\n",
    "    print(all_text[i])\n",
    "    print(\"***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
