{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process all the data\n",
    "\n",
    "Running the code cell below will pre-process all the data and save it to file. You're encouraged to lok at the code for `preprocess_and_save_data` in the `helpers.py` file to see what it's doing in detail, but you do not need to change this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus reader:\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import os\n",
    "root = './Confs_newline/Conf2/'\n",
    "from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n",
    "reader = CategorizedPlaintextCorpusReader(root, r'.*\\.txt', cat_pattern=r'(\\w+)/*', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kiz', 'kork', 'mutlu', 'notr', 'uzul']\n",
      "['kiz.txt', 'kork.txt', 'mutlu.txt', 'notr.txt', 'uzul.txt']\n"
     ]
    }
   ],
   "source": [
    "print(reader.categories())\n",
    "print(reader.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First, tokenize Punctuation: \n",
    "# create a token dictionary:\n",
    "punc_dict= {'.':'||PERIOD||', ',': '||COMMA||', '\"': '||QUOTATION_MARK||', ';': '||SEMICOLON||',\n",
    "                '!': '||EXCLAMATION_MARK||', '?': '||QUESTION_MARK||', '(': '||LEFT_PAREN||',\n",
    "                ')': '||RIGHT_PAREN||', '?': '||QUESTION_MARK||', \n",
    "                '\\n': '||NEW_LINE||', '-': '||DASH||'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenize_whole_tweets(text): # raw text --> whole tweets file content\n",
    "    for key, token in punc_dict.items():\n",
    "        text = text.replace(key, ' {} '.format(token))\n",
    "\n",
    "    sentences= []\n",
    "    for line in text.split('||NEW_LINE||'):\n",
    "        line= line.strip()\n",
    "        sentences.append(line)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text=[]\n",
    "labels= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3317\n",
      "3317\n"
     ]
    }
   ],
   "source": [
    "for label,file_name in zip(reader.categories(), reader.fileids()):\n",
    "    sentences= sent_tokenize_whole_tweets(reader.raw(file_name)) # --> this should return a list of contents\n",
    "    labels.extend([label for i in sentences])\n",
    "    all_text.extend([i.lower() for i in sentences])\n",
    "print(len(labels))\n",
    "print(len(all_text))\n",
    "# Now, we have all tweets in all_text list!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Transforming Text into Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 3704 \n"
     ]
    }
   ],
   "source": [
    "word_counts={}\n",
    "for i in range(len(all_text)):\n",
    "    for word in all_text[i].split(\" \"):\n",
    "        word_counts[word] = word_counts.get(word,0) +1\n",
    "\n",
    "vocab = set(word_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "print(\"Number of unique words: {} \".format(vocab_size))\n",
    "\n",
    "sorted_word_counts= sorted(word_counts, key= word_counts.get, reverse= True)\n",
    "\n",
    "int_to_vocab= {ii: word for ii,word in enumerate(sorted_word_counts)}\n",
    "vocab_to_int= {word: ii for ii, word in int_to_vocab.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hayır çok sev dizi yani aşırı kız'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'çok'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_vocab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "696"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts['çok']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing Noise in the Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling\n",
    "\n",
    "Words that show up often such as \"the\", \"of\", and \"for\" don't provide much context to the nearby words. If we discard some of them, we can remove some of the noise from our data and in return get faster training and better representations. This process is called subsampling by Mikolov. For each word $w_i$ in the training set, we'll discard it with probability given by \n",
    "\n",
    "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} $$\n",
    "\n",
    "where $t$ is a threshold parameter and $f(w_i)$ is the frequency of word $w_i$ in the total dataset.\n",
    "\n",
    "$$ P(0) = 1 - \\sqrt{\\frac{1*10^{-5}}{1*10^6/16*10^6}} = 0.98735 $$\n",
    "\n",
    "I'm going to leave this up to you as an exercise. Check out my solution to see how I did it.\n",
    "\n",
    "> **Exercise:** Implement subsampling for the words in `int_words`. That is, go through `int_words` and discard each word given the probablility $P(w_i)$ shown above. Note that $P(w_i)$ is the probability that a word is discarded. Assign the subsampled data to `train_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_int['her']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 1690, 1785, 2597, 2884]\n"
     ]
    }
   ],
   "source": [
    "int_words = [vocab_to_int[word] for word in vocab]\n",
    "print(int_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020, 1178, 1456, 150, 20, 17, 768, 263, 398, 291, 1179, 689, 2021, 230, 282, 106, 1180, 1457, 636, 166, 1181, 2022, 116, 578, 2023, 2024, 1006, 340, 277, 1458]\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-3\n",
    "\n",
    "word_counts_intwords = {vocab_to_int[word]:count for word,count in word_counts.items()}\n",
    "total_count = vocab_size\n",
    "freqs = {word: count/total_count for word, count in word_counts_intwords.items()}\n",
    "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts_intwords}\n",
    "# discard some frequent words, according to the subsampling equation\n",
    "# create a new list of words for training\n",
    "\n",
    "train_words = [word for word in word_counts_intwords if random.random() < (1 - p_drop[word])]\n",
    "print(train_words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3260"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3704"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while training the network, do not feed the words not in this list!\n",
    "# clean the tweets from those words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'çok kız ||exclamation_mark||  ne kadar ayıp şey'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_text)):\n",
    "    line_split= all_text[i].split(\" \")\n",
    "    line_split_reduced= [word for word in line_split if vocab_to_int[word] in train_words]\n",
    "    all_text[i]= ' '.join(line_split_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, delete empty tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3317"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3144\n",
      "3144\n"
     ]
    }
   ],
   "source": [
    "all_text_modified= [line for line in all_text if len(line)>0]\n",
    "labels_modified= [labels[i] for i in range(len(all_text)) if len(all_text[i])>0]\n",
    "print(len(all_text_modified))\n",
    "print(len(labels_modified))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the words\n",
    "The embedding lookup requires that we pass in integers to our network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Then we can convert each of our reviews into integers so they can be passed into the network.\n",
    "\n",
    "    Exercise: Now you're going to encode the words with integers. Build a dictionary that maps words to integers. Later we're going to pad our input vectors with zeros, so make sure the integers start at 1, not 0. Also, convert the reviews to integers and store the reviews in a new list called reviews_ints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 3260 \n"
     ]
    }
   ],
   "source": [
    "word_counts={}\n",
    "for i in range(len(all_text_modified)):\n",
    "    for word in all_text_modified[i].split(\" \"):\n",
    "        word_counts[word] = word_counts.get(word,0) +1\n",
    "\n",
    "vocab = set(word_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "print(\"Number of unique words: {} \".format(vocab_size))\n",
    "\n",
    "sorted_word_counts= sorted(word_counts, key= word_counts.get, reverse= True)\n",
    "\n",
    "int_to_vocab= {ii: word for ii,word in enumerate(sorted_word_counts, 1)} #start from 1.\n",
    "vocab_to_int= {word: ii for ii, word in int_to_vocab.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use the dict to tokenize each review in reviews_split\n",
    "## store the tokenized reviews in reviews_ints\n",
    "tweets_ints = []\n",
    "for tweet in all_text_modified:\n",
    "    tweets_ints.append([vocab_to_int[word] for word in tweet.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1577],\n",
       " [735, 1013, 25, 2, 1, 25, 365],\n",
       " [68],\n",
       " [133, 82, 736, 307, 1578, 54],\n",
       " [77, 15]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_ints[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the labels\n",
    "\n",
    "Our labels are \"positive\" or \"negative\". To use these labels in our network, we need to convert them to 0 and 1.\n",
    "\n",
    "    Exercise: Convert labels from positive and negative to 1 and 0, respectively, and place those in a new list, encoded_labels.\n",
    "\n",
    "# 1=positive, 0=negative label conversion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-5d8928a1096e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabels_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mencoded_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'positive'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels_split\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "labels_split = labels.split('\\n')\n",
    "\n",
    "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding sequences\n",
    "\n",
    "To deal with both short and very long reviews, we'll pad or truncate all our reviews to a specific length. For reviews shorter than some seq_length, we'll pad with 0s. For reviews longer than seq_length, we can truncate them to the first seq_length words. A good seq_length, in this case, is 200.\n",
    "\n",
    "    Exercise: Define a function that returns an array features that contains the padded data, of a standard size, that we'll pass to the network.\n",
    "\n",
    "        The data should come from review_ints, since we want to feed integers to the network.\n",
    "        Each row should be seq_length elements long.\n",
    "        For reviews shorter than seq_length words, left pad with 0s. That is, if the review is ['best', 'movie', 'ever'], [117, 18, 128] as integers, the row will look like [0, 0, 0, ..., 0, 117, 18, 128].\n",
    "        For reviews longer than seq_length, use only the first seq_length words as the feature vector.\n",
    "\n",
    "As a small example, if the seq_length=10 and an input review is:\n",
    "\n",
    "[117, 18, 128]\n",
    "\n",
    "The resultant, padded sequence should be:\n",
    "\n",
    "[0, 0, 0, 0, 0, 0, 0, 117, 18, 128]\n",
    "\n",
    "Your final features array should be a 2D array, with as many rows as there are reviews, and as many columns as the specified seq_length.\n",
    "\n",
    "This isn't trivial and there are a bunch of ways to do this. But, if you're going to be building your own deep learning networks, you're going to have to get used to preparing your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
