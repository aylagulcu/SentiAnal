{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kiz', 'kork', 'mutlu', 'notr', 'uzul']\n",
      "['kiz.txt', 'kork.txt', 'mutlu.txt', 'notr.txt', 'uzul.txt']\n",
      "3316\n",
      "3316\n"
     ]
    }
   ],
   "source": [
    "%run read_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['çok kız ne kadar ayıp şey',\n",
       " 'koku çok kız çünkü ayva ben tek arkadaş  insan var kendi git gibi tek arkadaş götür yan',\n",
       " 'hayır çok sev dizi yani aşırı kız',\n",
       " 'buna kız de evet ona kız ney kız kızneg sen mantık göre belirleneg kusur bakneg',\n",
       " 'kız ve çok üzül',\n",
       " 'kız bu geç ama aynı zaman kır bu nasıl geç',\n",
       " 'of ben yet çok kız karakter  ne güncelneg yap herkes vere zor ne adam',\n",
       " 'bak kendi çok kız şu sen muhatap al için oku için bekle',\n",
       " 'sonra durul küs kız biz bu kadar gönül bu takım sar o herkes met et futbol defans ton',\n",
       " 'evet çok sev hem de gül bir durum görneg sine çok kız']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train - test splits\n",
    "# test data will never be seen by the classifiers. \n",
    "# This left-out data will be used to test the NN as well.\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# parameters #\n",
    "train_ratio= 0.8\n",
    "test_ratio= 0.2\n",
    "SEED= 1\n",
    "random.seed= SEED\n",
    "#########\n",
    "\n",
    "all_data_count = len(all_text)\n",
    "indices = list(range(all_data_count))\n",
    "np.random.shuffle(indices) # first, shuffle the data! Do not miss this part!\n",
    "\n",
    "split = int(np.floor(test_ratio * all_data_count))\n",
    "train_idx, test_idx = indices[split:], indices[:split]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data will never be seen by the classifiers. This left-out data will be used to test the NN as well.\n",
    "\n",
    "all_text_train= [all_text[i] for i in train_idx]\n",
    "labels_train= [labels[i] for i in train_idx]\n",
    "all_text_test= [all_text[i] for i in test_idx]\n",
    "labels_train=  [labels[i] for i in test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of alt kendi adım gör hayat küs'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3323\n"
     ]
    }
   ],
   "source": [
    "# create words counts dictionary and the vocabulary:\n",
    "total_counts={}\n",
    "for i in range(len(all_text_train)):\n",
    "    for word in all_text_train[i].split(\" \"):\n",
    "        total_counts[word] = total_counts.get(word,0) +1\n",
    "\n",
    "vocab = set(total_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_0 = np.zeros((1,vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of words in the vocabulary mapped to index positions \n",
    "# (to be used in layer_0)\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "    \n",
    "# display the map of words to indices\n",
    "word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_input_layer(review):\n",
    "    \"\"\" Modify the global layer_0 to represent the vector form of review.\n",
    "    The element at a given index of layer_0 should represent\n",
    "    how many times the given word occurs in the review.\n",
    "    Args:\n",
    "        review(string) - the string of the review\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "     \n",
    "    global layer_0\n",
    "    \n",
    "    # clear out previous state, reset the layer to be all 0s\n",
    "    layer_0 *= 0\n",
    "    \n",
    "    # count how many times each word is used in the given review and store the results in layer_0 \n",
    "    for word in review.split(\" \"):\n",
    "        layer_0[0][word2index[word]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_input_layer(all_text_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_for_label(label):\n",
    "    if(label == 'kiz'):\n",
    "        return 1\n",
    "    elif (label == 'kork'):\n",
    "        return 2\n",
    "    elif (label == 'mutlu'):\n",
    "        return 3\n",
    "    elif (label == 'uzul'):\n",
    "        return 4\n",
    "    else:\n",
    "        return 0 # neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_target_for_label(labels_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of alt kendi adım gör hayat küs'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network\n",
    "- Do **not** add a non-linearity in the hidden layer. That is, do not use an activation function when calculating the hidden layer outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Encapsulate our neural network in a class\n",
    "class SentimentNetwork:\n",
    "    def __init__(self, reviews,labels,hidden_nodes = 10, learning_rate = 0.1):\n",
    "        \"\"\"Create a SentimenNetwork with the given settings\n",
    "        Args:\n",
    "            reviews(list) - List of reviews used for training\n",
    "            labels(list) - List of POSITIVE/NEGATIVE labels associated with the given reviews\n",
    "            hidden_nodes(int) - Number of nodes to create in the hidden layer\n",
    "            learning_rate(float) - Learning rate to use while training\n",
    "        \n",
    "        \"\"\"\n",
    "        # Assign a seed to our random number generator to ensure we get\n",
    "        # reproducable results during development \n",
    "        np.random.seed(1)\n",
    "\n",
    "        # process the reviews and their associated labels so that everything\n",
    "        # is ready for training\n",
    "        self.pre_process_data(reviews, labels)\n",
    "        \n",
    "        # Build the network to have the number of hidden nodes and the learning rate that\n",
    "        # were passed into this initializer. Make the same number of input nodes as\n",
    "        # there are vocabulary words and create a single output node.\n",
    "        self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate)\n",
    "\n",
    "    def pre_process_data(self, reviews, labels):\n",
    "        \n",
    "        # populate review_vocab with all of the words in the given reviews\n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(\" \"):\n",
    "                review_vocab.add(word)\n",
    "\n",
    "        # Convert the vocabulary set to a list so we can access words via indices\n",
    "        self.review_vocab = list(review_vocab)\n",
    "        \n",
    "        # populate label_vocab with all of the words in the given labels.\n",
    "        label_vocab = set()\n",
    "        for label in labels:\n",
    "            label_vocab.add(label)\n",
    "        \n",
    "        # Convert the label vocabulary set to a list so we can access labels via indices\n",
    "        self.label_vocab = list(label_vocab)\n",
    "        \n",
    "        # Store the sizes of the review and label vocabularies.\n",
    "        self.review_vocab_size = len(self.review_vocab)\n",
    "        self.label_vocab_size = len(self.label_vocab)\n",
    "        \n",
    "        # Create a dictionary of words in the vocabulary mapped to index positions\n",
    "        self.word2index = {}\n",
    "        for i, word in enumerate(self.review_vocab):\n",
    "            self.word2index[word] = i\n",
    "        \n",
    "        # Create a dictionary of labels mapped to index positions\n",
    "        self.label2index = {}\n",
    "        for i, label in enumerate(self.label_vocab):\n",
    "            self.label2index[label] = i\n",
    "        \n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set number of nodes in input, hidden and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Store the learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weights\n",
    "\n",
    "        # These are the weights between the input layer and the hidden layer.\n",
    "        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))\n",
    "    \n",
    "        # These are the weights between the hidden layer and the output layer.\n",
    "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, \n",
    "                                                (self.hidden_nodes, self.output_nodes))\n",
    "        \n",
    "        # The input layer, a two-dimensional matrix with shape 1 x input_nodes\n",
    "        self.layer_0 = np.zeros((1,input_nodes))\n",
    "    \n",
    "    def update_input_layer(self,review):\n",
    "\n",
    "        # clear out previous state, reset the layer to be all 0s\n",
    "        self.layer_0 *= 0\n",
    "        \n",
    "        for word in review.split(\" \"):\n",
    "            # NOTE: This if-check was not in the version of this method created in Project 2,\n",
    "            #       and it appears in Andrew's Project 3 solution without explanation. \n",
    "            #       It simply ensures the word is actually a key in word2index before\n",
    "            #       accessing it, which is important because accessing an invalid key\n",
    "            #       with raise an exception in Python. This allows us to ignore unknown\n",
    "            #       words encountered in new reviews.\n",
    "            if(word in self.word2index.keys()):\n",
    "                self.layer_0[0][self.word2index[word]] += 1\n",
    "                     \n",
    "    def get_target_for_label(self, label):\n",
    "        if(label == 'kiz'):\n",
    "            return 1\n",
    "        elif (label == 'kork'):\n",
    "            return 2\n",
    "        elif (label == 'mutlu'):\n",
    "            return 3\n",
    "        elif (label == 'uzul'):\n",
    "            return 4\n",
    "        else:\n",
    "            return 0 # neutral\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def softmax(x):\n",
    "        return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1, 1)\n",
    "    \n",
    "    def sigmoid_output_2_derivative(self,output):\n",
    "        return output * (1 - output)\n",
    "    \n",
    "    def train(self, training_reviews, training_labels):\n",
    "        \n",
    "        # make sure out we have a matching number of reviews and labels\n",
    "        assert(len(training_reviews) == len(training_labels))\n",
    "        \n",
    "        # Keep track of correct predictions to display accuracy during training \n",
    "        correct_so_far = 0\n",
    "\n",
    "        # Remember when we started for printing time statistics\n",
    "        start = time.time()\n",
    "        \n",
    "        # loop through all the given reviews and run a forward and backward pass,\n",
    "        # updating weights for every item\n",
    "        for i in range(len(training_reviews)):\n",
    "            \n",
    "            # Get the next review and its correct label\n",
    "            review = training_reviews[i]\n",
    "            label = training_labels[i]\n",
    "            \n",
    "            #### Implement the forward pass here ####\n",
    "            ### Forward pass ###\n",
    "\n",
    "            # Input Layer\n",
    "            self.update_input_layer(review)\n",
    "\n",
    "            # Hidden layer\n",
    "            layer_1 = self.layer_0.dot(self.weights_0_1)\n",
    "\n",
    "            # Output layer\n",
    "            layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))\n",
    "            \n",
    "            #### Implement the backward pass here ####\n",
    "            ### Backward pass ###\n",
    "\n",
    "            # Output error\n",
    "            layer_2_error = layer_2 - self.get_target_for_label(label) # Output layer error is the difference between desired target and actual output.\n",
    "            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)\n",
    "\n",
    "            # Backpropagated error\n",
    "            layer_1_error = layer_2_delta.dot(self.weights_1_2.T) # errors propagated to the hidden layer\n",
    "            layer_1_delta = layer_1_error # hidden layer gradients - no nonlinearity so it's the same as the error\n",
    "\n",
    "            # Update the weights\n",
    "            self.weights_1_2 -= layer_1.T.dot(layer_2_delta) * self.learning_rate # update hidden-to-output weights with gradient descent step\n",
    "            self.weights_0_1 -= self.layer_0.T.dot(layer_1_delta) * self.learning_rate # update input-to-hidden weights with gradient descent step\n",
    "\n",
    "            # Keep track of correct predictions.\n",
    "            if(layer_2 >= 0.5 and label == 'POSITIVE'):\n",
    "                correct_so_far += 1\n",
    "            elif(layer_2 < 0.5 and label == 'NEGATIVE'):\n",
    "                correct_so_far += 1\n",
    "            \n",
    "            # For debug purposes, print out our prediction accuracy and speed \n",
    "            # throughout the training process. \n",
    "            elapsed_time = float(time.time() - start)\n",
    "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                             + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) \\\n",
    "                             + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
    "            if(i % 2500 == 0):\n",
    "                print(\"\")\n",
    "    \n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        \"\"\"\n",
    "        Attempts to predict the labels for the given testing_reviews,\n",
    "        and uses the test_labels to calculate the accuracy of those predictions.\n",
    "        \"\"\"\n",
    "        \n",
    "        # keep track of how many correct predictions we make\n",
    "        correct = 0\n",
    "\n",
    "        # we'll time how many predictions per second we make\n",
    "        start = time.time()\n",
    "\n",
    "        # Loop through each of the given reviews and call run to predict\n",
    "        # its label. \n",
    "        for i in range(len(testing_reviews)):\n",
    "            pred = self.run(testing_reviews[i])\n",
    "            if(pred == testing_labels[i]):\n",
    "                correct += 1\n",
    "            \n",
    "            # For debug purposes, print out our prediction accuracy and speed \n",
    "            # throughout the prediction process. \n",
    "\n",
    "            elapsed_time = float(time.time() - start)\n",
    "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                             + \" #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) \\\n",
    "                             + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
    "    \n",
    "    def run(self, review):\n",
    "        \"\"\"\n",
    "        Returns a POSITIVE or NEGATIVE prediction for the given review.\n",
    "        \"\"\"\n",
    "        # Run a forward pass through the network, like in the \"train\" function.\n",
    "        \n",
    "        # Input Layer\n",
    "        self.update_input_layer(review.lower())\n",
    "\n",
    "        # Hidden layer\n",
    "        layer_1 = self.layer_0.dot(self.weights_0_1)\n",
    "\n",
    "        # Output layer\n",
    "        layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))\n",
    "        \n",
    "        # Return POSITIVE for values above greater-than-or-equal-to 0.5 in the output layer;\n",
    "        # return NEGATIVE for other values\n",
    "        if(layer_2[0] >= 0.5):\n",
    "            return \"POSITIVE\"\n",
    "        else:\n",
    "            return \"NEGATIVE\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "fold 1 accuracy: 0.8345864661654135\n",
      "fold 2 accuracy: 0.8463855421686747\n",
      "fold 3 accuracy: 0.8491704374057315\n",
      "fold 4 accuracy: 0.8235294117647058\n",
      "fold 5 accuracy: 0.8350983358547656\n",
      "Mean score: 0.8377540386718583\n",
      " \n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "fold 1 accuracy: 0.8661654135338346\n",
      "fold 2 accuracy: 0.8780120481927711\n",
      "fold 3 accuracy: 0.8778280542986425\n",
      "fold 4 accuracy: 0.8778280542986425\n",
      "fold 5 accuracy: 0.8774583963691377\n",
      "Mean score: 0.8754583933386056\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# WITH UNIGRAM COUNT VECTORIZER:\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "# The folds are made by preserving the percentage of samples for each class.\n",
    "# use the original data, all_text!\n",
    "\n",
    "list_classifier= [MultinomialNB(), LinearSVC()]\n",
    "\n",
    "for clf in list_classifier:\n",
    "    print(clf)\n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True, random_state=123)\n",
    "    all_text= np.array(all_text)\n",
    "    labels= np.array(labels)\n",
    "    scores= []\n",
    "    i= 1\n",
    "    for train_index, test_index in skf.split(all_text, labels):\n",
    "        X_train, y_train = all_text[train_index], labels[train_index] \n",
    "        X_test, y_test = all_text[test_index], labels[test_index]\n",
    "\n",
    "        train_vectorizer = CountVectorizer()\n",
    "        X_train = train_vectorizer.fit_transform(X_train)\n",
    "        train_vocab= train_vectorizer.vocabulary_   \n",
    "        test_vectorizer = CountVectorizer(vocabulary=train_vocab)\n",
    "        X_test = test_vectorizer.fit_transform(X_test)\n",
    "        clf.fit(X_train, y_train)\n",
    "        sc= accuracy_score(y_test, clf.predict(X_test))\n",
    "        scores.append(sc)\n",
    "        print(\"fold \"+ str(i)+ \" accuracy: \"+ str(sc))\n",
    "        i+= 1\n",
    "    print(\"Mean score: \"+ str(np.mean(scores)))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "fold 1 accuracy: 0.8\n",
      "fold 2 accuracy: 0.7966867469879518\n",
      "fold 3 accuracy: 0.8054298642533937\n",
      "fold 4 accuracy: 0.77526395173454\n",
      "fold 5 accuracy: 0.81089258698941\n",
      "Mean score: 0.7976546299930591\n",
      " \n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "fold 1 accuracy: 0.8857142857142857\n",
      "fold 2 accuracy: 0.8855421686746988\n",
      "fold 3 accuracy: 0.8868778280542986\n",
      "fold 4 accuracy: 0.8883861236802413\n",
      "fold 5 accuracy: 0.8835098335854765\n",
      "Mean score: 0.8860060479418002\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# WITH UNIGRAM TF VECTORIZER:\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "# The folds are made by preserving the percentage of samples for each class.\n",
    "# use the original data, all_text!\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "list_classifier= [MultinomialNB(), LinearSVC()]\n",
    "\n",
    "for clf in list_classifier:\n",
    "    print(clf)\n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True, random_state=123)\n",
    "    all_text= np.array(all_text)\n",
    "    labels= np.array(labels)\n",
    "    scores= []\n",
    "    i= 1\n",
    "    for train_index, test_index in skf.split(all_text, labels):\n",
    "        X_train, y_train = all_text[train_index], labels[train_index] \n",
    "        X_test, y_test = all_text[test_index], labels[test_index]\n",
    "\n",
    "        train_vectorizer = CountVectorizer()\n",
    "        X_train = train_vectorizer.fit_transform(X_train)\n",
    "        transformer_tf= TfidfTransformer(use_idf= False, norm= 'l2') # if normalize None, it is the same as CountVect\n",
    "        # l2 normalization: like percentage of values\n",
    "        X_train= transformer_tf.fit_transform(X_train)\n",
    "           \n",
    "        train_vocab= train_vectorizer.vocabulary_   \n",
    "        test_vectorizer = CountVectorizer(vocabulary=train_vocab)\n",
    "        X_test = test_vectorizer.fit_transform(X_test)\n",
    "        X_test= transformer_tf.fit_transform(X_test)\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        sc= accuracy_score(y_test, clf.predict(X_test))\n",
    "        scores.append(sc)\n",
    "        print(\"fold \"+ str(i)+ \" accuracy: \"+ str(sc))\n",
    "        i+= 1\n",
    "    print(\"Mean score: \"+ str(np.mean(scores)))\n",
    "    print(\" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "fold 1 accuracy: 0.8\n",
      "fold 2 accuracy: 0.8012048192771084\n",
      "fold 3 accuracy: 0.8069381598793364\n",
      "fold 4 accuracy: 0.7782805429864253\n",
      "fold 5 accuracy: 0.8093797276853253\n",
      "Mean score: 0.799160649965639\n",
      " \n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "fold 1 accuracy: 0.8872180451127819\n",
      "fold 2 accuracy: 0.8855421686746988\n",
      "fold 3 accuracy: 0.8838612368024132\n",
      "fold 4 accuracy: 0.8838612368024132\n",
      "fold 5 accuracy: 0.8789712556732224\n",
      "Mean score: 0.8838907886131059\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# WITH UNIGRAM TF-idf VECTORIZER:\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "# The folds are made by preserving the percentage of samples for each class.\n",
    "# use the original data, all_text!\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "list_classifier= [MultinomialNB(), LinearSVC()]\n",
    "\n",
    "for clf in list_classifier:\n",
    "    print(clf)\n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True, random_state=123)\n",
    "    all_text= np.array(all_text)\n",
    "    labels= np.array(labels)\n",
    "    scores= []\n",
    "    i= 1\n",
    "    for train_index, test_index in skf.split(all_text, labels):\n",
    "        X_train, y_train = all_text[train_index], labels[train_index] \n",
    "        X_test, y_test = all_text[test_index], labels[test_index]\n",
    "\n",
    "        train_vectorizer = CountVectorizer()\n",
    "        X_train = train_vectorizer.fit_transform(X_train)\n",
    "        transformer_tf= TfidfTransformer(use_idf= True, norm= 'l2', smooth_idf=True) \n",
    "        # if normalize None, it is the same as CountVect\n",
    "        # l2 normalization: like percentage of values\n",
    "        X_train= transformer_tf.fit_transform(X_train)\n",
    "           \n",
    "        train_vocab= train_vectorizer.vocabulary_   \n",
    "        test_vectorizer = CountVectorizer(vocabulary=train_vocab)\n",
    "        X_test = test_vectorizer.fit_transform(X_test)\n",
    "        X_test= transformer_tf.fit_transform(X_test)\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        sc= accuracy_score(y_test, clf.predict(X_test))\n",
    "        scores.append(sc)\n",
    "        print(\"fold \"+ str(i)+ \" accuracy: \"+ str(sc))\n",
    "        i+= 1\n",
    "    print(\"Mean score: \"+ str(np.mean(scores)))\n",
    "    print(\" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
